{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torchvision.datasets\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch.nn.functional as F\nfrom torchvision.datasets import CIFAR10\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\nfrom glob import glob\nfrom tqdm import tqdm\nfrom copy import deepcopy\nimport pickle\nfrom cifar10_utils import get_cifar10, get_dataloader\n\nfrom models_v2 import (BasicCNN, AutoencoderCNN, ECACNN, AutoencoderECACNN,\n                       ECASpatialCNN, DeeperCNN, LinearAutoencoderECACNN, CBAMCNN)\n","metadata":{"id":"20i9dQgTTahN","execution":{"iopub.status.busy":"2023-07-21T10:03:08.430106Z","iopub.execute_input":"2023-07-21T10:03:08.430723Z","iopub.status.idle":"2023-07-21T10:03:12.586237Z","shell.execute_reply.started":"2023-07-21T10:03:08.430690Z","shell.execute_reply":"2023-07-21T10:03:12.585091Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Seed for reproduceability\nseed = 42\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.determinstic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(42)\n\n# Setup device-agnostic code\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nuse_cuda = torch.cuda.is_available()\nprint(device)","metadata":{"id":"7M0lsnssT8B5","outputId":"253b6e3e-9cc4-4055-872f-8ea7bf1b5b6a","execution":{"iopub.status.busy":"2023-07-21T10:03:12.588262Z","iopub.execute_input":"2023-07-21T10:03:12.588824Z","iopub.status.idle":"2023-07-21T10:03:12.602595Z","shell.execute_reply.started":"2023-07-21T10:03:12.588794Z","shell.execute_reply":"2023-07-21T10:03:12.601036Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"# Grad-CAM: for more info https://jacobgil.github.io/pytorch-gradcam-book/introduction.html\n%pip install grad-cam --quiet\nfrom pytorch_grad_cam import GradCAM\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom pytorch_grad_cam.utils.image import show_cam_on_image","metadata":{"id":"pAGh5wv7U6_l","outputId":"adbe3a8f-ef01-4dde-d405-4fdb6a31298a","execution":{"iopub.status.busy":"2023-07-21T10:03:18.200012Z","iopub.execute_input":"2023-07-21T10:03:18.200235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cifar10 = get_cifar10()\ncifar10_loader = get_dataloader(cifar10, 128)","metadata":{"id":"lGQcyxykmy76","outputId":"6b1154a6-2add-4a82-a5c1-36977b4acc46","execution":{"iopub.status.busy":"2023-07-21T10:03:12.604198Z","iopub.execute_input":"2023-07-21T10:03:12.604523Z","iopub.status.idle":"2023-07-21T10:03:18.187139Z","shell.execute_reply.started":"2023-07-21T10:03:12.604493Z","shell.execute_reply":"2023-07-21T10:03:18.186432Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:02<00:00, 63356698.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting data/cifar-10-python.tar.gz to data/\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"def make_plots(logging_dict, model_name, avg_train=True):\n#     logging_dict = {'loss': {'train': [], 'validation': []},\n#                 'accuracy': {'train': [], 'validation': []},\n#                 'lr': [],\n#                 'batches_per_epoch': [],}\n    epoch_ends = np.cumsum(logging_dict['batches_per_epoch'])\n\n    def get_avg_per_epoch(batch_data):\n        result = [None,]\n        for i in range(len(epoch_ends) - 1):\n            result.append(np.average(batch_data[epoch_ends[i]:epoch_ends[i + 1]]))\n        return result\n\n    fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n    metrics = ('loss', 'accuracy')\n    for metric, ax in zip(metrics, axes.ravel()):\n#         ax.plot(logging_dict[metric]['train'])\n        if avg_train:\n            ax.plot(get_avg_per_epoch(logging_dict[metric]['train']), '.-', label='training set')\n            ax.plot(logging_dict[metric]['validation'], '.-', label='validation set')\n            ax.set(title=metric, xlabel='epoch', xticks=np.arange(len(epoch_ends)))\n        else:\n            ax.plot(logging_dict[metric]['train'],'.-', label='training set')\n            ax.plot(epoch_ends, logging_dict[metric]['validation'],'.-', label='validation set')\n            ax.set(title=metric, xlabel='batch')\n\n    handles, labels = ax.get_legend_handles_labels()\n    plt.figlegend(handles=handles, labels=labels, loc='upper center', bbox_to_anchor=(0.5, 0), ncol=2)\n    plt.suptitle(model_name)\n    plt.tight_layout()\n    plt.show()","metadata":{"id":"cfeGG_jQUDDh","execution":{"iopub.status.busy":"2023-07-21T10:03:18.188829Z","iopub.execute_input":"2023-07-21T10:03:18.189292Z","iopub.status.idle":"2023-07-21T10:03:18.199090Z","shell.execute_reply.started":"2023-07-21T10:03:18.189267Z","shell.execute_reply":"2023-07-21T10:03:18.197952Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"for file in glob('../input/models-data/models/*/*'):\n    print(file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_model = BasicCNN().to(device)\nautoencoder_model = AutoencoderCNN().to(device)\neca_model = ECACNN().to(device)\nautoencoder_eca_model = AutoencoderECACNN().to(device)\neca_spatial_model = ECASpatialCNN().to(device)\ndeeper_cnn_model = DeeperCNN().to(device)\nlinear_autoencoder_model = LinearAutoencoderECACNN().to(device)\ncbam_model = CBAMCNN().to(device)\n","metadata":{"id":"YGlZCa_LYlo8","outputId":"604772f5-f73e-41db-9d16-d877cb017642","execution":{"iopub.status.busy":"2023-07-21T09:57:24.402659Z","iopub.execute_input":"2023-07-21T09:57:24.403846Z","iopub.status.idle":"2023-07-21T09:57:24.749066Z","shell.execute_reply.started":"2023-07-21T09:57:24.403800Z","shell.execute_reply":"2023-07-21T09:57:24.747737Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"models = {'cnn': {'instance': cnn_model, 'data': None},\n          'eca': {'instance': eca_model, 'data': None},\n          'cbam': {'instance': cbam_model, 'data': None},\n          'autoencoder': {'instance': autoencoder_model, 'data': None},\n          'autoencoder_eca': {'instance': autoencoder_eca_model, 'data': None},\n          'linear_autoencoder_eca': {'instance': linear_autoencoder_model, 'data': None},\n          'eca_spatial': {'instance': eca_spatial_model, 'data': None},\n          'deeper_cnn': {'instance': deeper_cnn_model, 'data': None},\n}\n\n\nfor name, model in models.items():\n    model['instance'].load_state_dict(torch.load(f'../input/models-data/models/{name}/{name}.pt', map_location=torch.device(device)))\n    with open(f'../input/models-data/models/{name}/{name}.pkl', 'rb') as f:\n        model['data'] = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2023-07-21T09:57:24.750702Z","iopub.execute_input":"2023-07-21T09:57:24.751181Z","iopub.status.idle":"2023-07-21T09:57:26.530218Z","shell.execute_reply.started":"2023-07-21T09:57:24.751136Z","shell.execute_reply":"2023-07-21T09:57:26.528640Z"},"trusted":true},"execution_count":8,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m models \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnn\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstance\u001b[39m\u001b[38;5;124m'\u001b[39m: cnn_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m},\n\u001b[1;32m      2\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meca\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstance\u001b[39m\u001b[38;5;124m'\u001b[39m: eca_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m},\n\u001b[1;32m      3\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcbam\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstance\u001b[39m\u001b[38;5;124m'\u001b[39m: cbam_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeeper_cnn\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstance\u001b[39m\u001b[38;5;124m'\u001b[39m: deeper_cnn_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m},\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 13\u001b[0m     model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstance\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../input/models-data/models/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../input/models-data/models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     15\u001b[0m         model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/models-data/models/linear_autoencoder_eca/linear_autoencoder_eca.pt'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '../input/models-data/models/linear_autoencoder_eca/linear_autoencoder_eca.pt'","output_type":"error"}]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"classes_names = {i: c for i, c in enumerate (('airplane', 'automobile', 'bird', 'cat', 'deer',\n                                              'dog', 'frog', 'horse', 'ship', 'truck',))}\ndef compare_activations(models_dict: dict, target_layers, image_number=None, dataset=cifar10['train'], use_cuda=True):\n    '''\n    Compare gradcam visualizations of differenct models.\n    If image_number is None, a random image will be selected.\n    '''\n    fig, axs = plt.subplots(1, len(models) + 1, figsize=(15, 3))\n    if image_number == None:\n        image_number = np.random.randint(len(dataset))\n    img, label = cifar10['train'][image_number]\n        \n    def normalize(img):\n        return ((img - img.min()) / (img.max() - img.min()))\n    \n    def inverse_normalize(img):\n        invTrans = transforms.Compose([ transforms.Normalize(mean = [ 0., 0., 0. ],\n                                                     std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n                                transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n                                                     std = [ 1., 1., 1. ]),\n                               ])\n        return invTrans(img)\n\n    \n    # Show original image\n    axs = axs.ravel()\n    axs[0].imshow(torch.permute(inverse_normalize(img), (1, 2, 0)))\n    axs[0].axis('off')\n    axs[0].set(title='Original Image')\n    \n    # Show activations for each model\n    for i, (model, ax) in enumerate(zip(models_dict.items(), axs[1:])):\n        model_name = model[0]\n        model_instance = model[1]['instance']\n#         target_layer = [model_instance.get_submodule(target_layers[model_name])]\n        target_layer = [model_instance.get_submodule('layer2')]\n\n        model_instance.eval()\n        input_tensor = img.unsqueeze(0).to(device)       \n        cam = GradCAM(model=model_instance, target_layers=target_layer, use_cuda=use_cuda)\n        targets = [ClassifierOutputTarget(label)]\n        targets= None\n        grayscale_cam = cam(input_tensor, targets)\n        grayscale_cam = grayscale_cam[0, :]\n        visualization = show_cam_on_image(normalize(torch.permute(img, (1, 2, 0))).numpy(), grayscale_cam, use_rgb=True)\n        ax.imshow(visualization)\n        ax.axis('off')\n        ax.set(title=f'{model_name}')\n        \n        # Display classifiction results\n        \n        with torch.no_grad():\n            probabilities = model_instance(input_tensor)\n        results = [(classes_names[index], prob) for index, prob in enumerate(probabilities.squeeze(0))]\n        results_sorted = sorted(results, key=lambda item: item[1], reverse=True)\n        results_text = '\\n'.join([f'{res[0]} ({res[1].item():.2f})' for res in results_sorted[:3]])\n        ax.annotate(results_text, (0.5, -0.1), (0, 0), xycoords='axes fraction', textcoords='offset points', va='top', ha='center');\n    \n    plt.suptitle(f'{classes_names[label]} ({label}) (image {image_number})')\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-21T09:48:13.484705Z","iopub.status.idle":"2023-07-21T09:48:13.485288Z","shell.execute_reply.started":"2023-07-21T09:48:13.485093Z","shell.execute_reply":"2023-07-21T09:48:13.485114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compare_activations(models_dict=models, target_layers=model_target_layers, dataset=cifar10['test'])","metadata":{"execution":{"iopub.status.busy":"2023-07-21T09:48:13.486196Z","iopub.status.idle":"2023-07-21T09:48:13.486658Z","shell.execute_reply.started":"2023-07-21T09:48:13.486512Z","shell.execute_reply":"2023-07-21T09:48:13.486528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"good:\n* 2612\n* 44974\n* 14541\n* 19457\n* 26557\n* 33763\n* 39504\n* 38660\n* 38158\n* 6265\n* 4658\n\nsame:\n* 1021\n* 11653\n* 43573\n* 13417\n* 9692\n\nbad:\n* 5675\n* 16850\n* 37194\n* 41090\n* 37819\n\nmeh:\n* 3561\n* 21962\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-20T11:41:12.299459Z","iopub.execute_input":"2023-07-20T11:41:12.299889Z","iopub.status.idle":"2023-07-20T11:41:12.307031Z","shell.execute_reply.started":"2023-07-20T11:41:12.299849Z","shell.execute_reply":"2023-07-20T11:41:12.305824Z"}}},{"cell_type":"code","source":"def topk_accuracy(output, target, topk=(1, 3)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n    ","metadata":{"execution":{"iopub.status.busy":"2023-07-21T09:48:13.487510Z","iopub.status.idle":"2023-07-21T09:48:13.487946Z","shell.execute_reply.started":"2023-07-21T09:48:13.487793Z","shell.execute_reply":"2023-07-21T09:48:13.487809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_plots(logging_dict, model_name, avg_train=True):\n#     logging_dict = {'loss': {'train': [], 'validation': []},\n#                 'accuracy': {'train': [], 'validation': []},\n#                 'lr': [],\n#                 'batches_per_epoch': [],}\n    epoch_ends = np.cumsum(logging_dict['batches_per_epoch'])\n\n    def get_avg_per_epoch(batch_data):\n        result = [None,]\n        for i in range(len(epoch_ends) - 1):\n            result.append(np.average(batch_data[epoch_ends[i]:epoch_ends[i + 1]]))\n        return result\n\n    fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n    metrics = ('loss', 'accuracy')\n    for metric, ax in zip(metrics, axes.ravel()):\n#         ax.plot(logging_dict[metric]['train'])\n        if avg_train:\n            ax.plot(get_avg_per_epoch(logging_dict[metric]['train']), '.-', label='training set')\n            ax.plot(logging_dict[metric]['validation'], '.-', label='validation set')\n            ax.set(title=metric, xlabel='epoch', xticks=np.arange(len(epoch_ends)))\n        else:\n            ax.plot(logging_dict[metric]['train'],'.-', label='training set')\n            ax.plot(epoch_ends, logging_dict[metric]['validation'],'.-', label='validation set')\n            ax.set(title=metric, xlabel='batch')\n\n    handles, labels = ax.get_legend_handles_labels()\n    plt.figlegend(handles=handles, labels=labels, loc='upper center', bbox_to_anchor=(0.5, 0), ncol=2)\n    plt.suptitle(model_name)\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-21T09:48:13.488762Z","iopub.status.idle":"2023-07-21T09:48:13.489190Z","shell.execute_reply.started":"2023-07-21T09:48:13.489054Z","shell.execute_reply":"2023-07-21T09:48:13.489068Z"},"trusted":true},"execution_count":null,"outputs":[]}]}