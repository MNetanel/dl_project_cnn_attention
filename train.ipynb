{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "from cifar10_utils import get_cifar10, get_dataloader\n",
        "\n",
        "from models_v2 import (BasicCNN, AutoencoderCNN, ECACNN, AutoencoderECACNN,\n",
        "                       ECASpatialCNN, DeeperCNN, LinearAutoencoderECACNN, CBAMCNN)"
      ],
      "metadata": {
        "id": "LPQ9g1l1Yypb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed for reproduceability\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.determinstic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(42)\n",
        "\n",
        "# Setup device-agnostic code\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bic8UF5wZBdX",
        "outputId": "65991547-d894-4981-d4f4-b977152beb0d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10 = get_cifar10()\n",
        "cifar10_loader = get_dataloader(cifar10, 128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKLA2C-0ZCtd",
        "outputId": "2af0aa8b-0b35-4727-bda0-264477dda3b0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:12<00:00, 13133635.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data/\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_loader):\n",
        "    \"\"\"\n",
        "    Performs the evaluation of the MLP model on a given dataset.\n",
        "\n",
        "    Args:\n",
        "      model: An instance of 'MLP', the model to evaluate.\n",
        "      data_loader: The data loader of the dataset to evaluate.\n",
        "    Returns:\n",
        "        accuracy\n",
        "    \"\"\"\n",
        "    accuracies_per_batch, losses_per_batch = [], []\n",
        "    loss_module = nn.CrossEntropyLoss()\n",
        "    # Get accuracy for epoch\n",
        "    for batch in data_loader:\n",
        "\n",
        "        # Get validation images and labels\n",
        "        X = batch[0].to(device)\n",
        "        y = batch[1].to(device)\n",
        "\n",
        "        # Get predictions on validation set\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred_logits = model.forward(X)\n",
        "            pred_classes = torch.argmax(torch.softmax(pred_logits, dim=1), axis=1)\n",
        "\n",
        "        # Calculate accuracy := # of correct preds / total # of preds\n",
        "        current_accuracy = torch.sum(pred_classes == y) / pred_classes.shape[0]\n",
        "        accuracies_per_batch.append(current_accuracy.item())\n",
        "        current_loss = loss_module(pred_logits, y).item()\n",
        "        losses_per_batch.append(current_loss)\n",
        "\n",
        "    accuracy = np.average(accuracies_per_batch)\n",
        "    loss = np.average(losses_per_batch)\n",
        "\n",
        "    return accuracy, loss"
      ],
      "metadata": {
        "id": "QHobtukYZD0f"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, epochs=10, lr=0.1, momentum=0, verbose=True):\n",
        "\n",
        "    logging_dict = {'loss': {'train': [], 'validation': []},\n",
        "                    'accuracy': {'train': [], 'validation': []},\n",
        "                    'lr': [],\n",
        "                    'batches_per_epoch': [],\n",
        "                    'momentum': momentum}\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "\n",
        "        batches_per_epoch = 0\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        # Loss module and optimizer\n",
        "        loss_module = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\n",
        "\n",
        "\n",
        "        for batch in cifar10_loader['train']:\n",
        "\n",
        "            batches_per_epoch += 1\n",
        "\n",
        "            # Get training images and labels\n",
        "            X_train = batch[0].to(device)\n",
        "            y_train = batch[1].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            train_pred_logits = model.forward(X_train)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_module(train_pred_logits, y_train)\n",
        "            logging_dict['loss']['train'].append(loss.item())\n",
        "\n",
        "            # Calculate accuracy\n",
        "            train_pred_class = torch.argmax(torch.softmax(train_pred_logits, dim=1), axis=1)\n",
        "            train_accuracy = torch.sum(train_pred_class == y_train) / train_pred_class.shape[0]\n",
        "            logging_dict['accuracy']['train'].append(train_accuracy.item())\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "        # Log num of batches for this epoch\n",
        "        logging_dict['batches_per_epoch'].append(batches_per_epoch)\n",
        "\n",
        "        # Log current LR\n",
        "        logging_dict['lr'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "\n",
        "        # Get metrics on validation set\n",
        "        validation_accuracy, validation_loss = evaluate_model(model, cifar10_loader['validation'])\n",
        "\n",
        "        # Update LR\n",
        "        scheduler.step(validation_loss)\n",
        "\n",
        "        # Determine if best model\n",
        "        if len(logging_dict['accuracy']['validation']) == 1 or \\\n",
        "            all([validation_accuracy > acc for acc in logging_dict['accuracy']['validation']]):\n",
        "            best_model = deepcopy(model)\n",
        "\n",
        "        logging_dict['accuracy']['validation'].append(validation_accuracy.item())\n",
        "        logging_dict['loss']['validation'].append(validation_loss.item())\n",
        "\n",
        "        if verbose:\n",
        "            print(f'\\n{epoch = }, '\n",
        "                  f'training accuracy: {train_accuracy.item():.3f}, '\n",
        "                  f'training loss: {loss.item():.3f}',\n",
        "                  f'validation accuracy: {validation_accuracy.item():.3f}, '\n",
        "                  f'validation loss: {validation_loss.item():.3f}',\n",
        "                 )\n",
        "\n",
        "    # Get metrics on test set\n",
        "    test_accuracy, test_loss = evaluate_model(best_model, cifar10_loader['test'])\n",
        "    if verbose:\n",
        "        print(f'test accuracy: {test_accuracy}, test loss: {test_loss}')\n",
        "\n",
        "    return best_model, test_accuracy, test_loss, logging_dict"
      ],
      "metadata": {
        "id": "9aLF-1coZNnG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_runs = 5\n",
        "\n",
        "results = {'cnn': [],\n",
        "            'eca': [],\n",
        "            'cbam': [],\n",
        "            'autoencoder': [],\n",
        "            'autoencoder_eca': [],\n",
        "            'linear_autoencoder_eca': [],\n",
        "            'eca_spatial': [],\n",
        "            'deeper_cnn': [],\n",
        "}\n",
        "\n",
        "for run in range(num_runs):\n",
        "    cnn_model = BasicCNN().to(device)\n",
        "    autoencoder_model = AutoencoderCNN().to(device)\n",
        "    eca_model = ECACNN().to(device)\n",
        "    autoencoder_eca_model = AutoencoderECACNN().to(device)\n",
        "    eca_spatial_model = ECASpatialCNN().to(device)\n",
        "    deeper_cnn_model = DeeperCNN().to(device)\n",
        "    linear_autoencoder_model = LinearAutoencoderECACNN().to(device)\n",
        "    cbam_model = CBAMCNN().to(device)\n",
        "\n",
        "\n",
        "    models = {\n",
        "        'cnn': cnn_model,\n",
        "        'eca': eca_model,\n",
        "        'cbam': cbam_model,\n",
        "        'autoencoder': autoencoder_model,\n",
        "        'autoencoder_eca': autoencoder_eca_model,\n",
        "        'linear_autoencoder_eca': linear_autoencoder_model,\n",
        "        'eca_spatial': eca_spatial_model,\n",
        "        'deeper_cnn': deeper_cnn_model,\n",
        "    }\n",
        "\n",
        "    # Kaiming initialization\n",
        "    def init_weights(m):\n",
        "        if hasattr(m, 'weight') and m.weight is not None and len(m.weight.shape) > 1:\n",
        "            nn.init.kaiming_normal_(m.weight)\n",
        "        if hasattr(m, 'bias') and m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "    for model in models.values():\n",
        "        model.apply(init_weights)\n",
        "\n",
        "\n",
        "    print(f'run {run + 1} / {num_runs}')\n",
        "    for name, model in models.items():\n",
        "        print(f'training {name}')\n",
        "        best_model, test_accuracy, test_loss, logging_dict = train(model, verbose=False, epochs=10)\n",
        "        results[name].append(\n",
        "            {name: {'best_model': best_model.state_dict(),\n",
        "            'test_accuracy': test_accuracy,\n",
        "            'test_loss': test_loss,\n",
        "            'logging_dict': logging_dict}})\n",
        "        print('done training.')"
      ],
      "metadata": {
        "id": "f5EyCKlIZO_K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}